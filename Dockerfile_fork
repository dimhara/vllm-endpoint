# --- Stage 1: Build llama.cpp ---
FROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS builder
ENV DEBIAN_FRONTEND=noninteractive

# 1. Install Build Dependencies
RUN apt-get update && apt-get install -y build-essential cmake git libcurl4-openssl-dev

# 2. FIX: Link CUDA stubs for building without a GPU driver
# This mimics the presence of the NVIDIA driver during the linking phase
RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1
ENV LD_LIBRARY_PATH="/usr/local/cuda/lib64/stubs:${LD_LIBRARY_PATH}"

# 3. Build llama-server
RUN git clone https://github.com/ggerganov/llama.cpp.git /llama.cpp && \
    cd /llama.cpp && \
    cmake -B build -DGGML_CUDA=ON && \
    cmake --build build --config Release -j$(nproc) --target llama-server

# --- Stage 2: Runtime ---
FROM nvidia/cuda:12.4.1-runtime-ubuntu22.04
ENV DEBIAN_FRONTEND=noninteractive

# 1. Install Python & Utils
RUN apt-get update && apt-get install -y python3 python3-pip curl && rm -rf /var/lib/apt/lists/*
RUN pip3 install --no-cache-dir runpod cryptography requests huggingface_hub hf_transfer

# 2. Copy the compiled binary
COPY --from=builder /llama.cpp/build/bin/llama-server /usr/local/bin/llama-server

# 3. Copy ALL required Shared Libraries
COPY --from=builder /llama.cpp/build/bin/*.so /usr/local/lib/

# 3. Tell Linux where to find them
ENV LD_LIBRARY_PATH="/usr/local/lib:${LD_LIBRARY_PATH}"



# 3. Setup App
WORKDIR /app
COPY . .

# 4. Make script executable
RUN chmod +x start_fork.sh

# 5. Environment
ENV HF_HUB_ENABLE_HF_TRANSFER=1
ENV MODEL_DIR=/models

# 6. Entrypoint
ENTRYPOINT []
CMD ["/bin/bash", "/app/start_fork.sh"]