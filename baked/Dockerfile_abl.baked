# 1. Start from your previously built logic image
FROM ghcr.io/dimhara/llamacpp-endpoint:latest

# 2. Define the model to bake in at build time
# RunPod allows passing build arguments, or you can hardcode it here.
ARG BAKE_MODELS="DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf:OpenAI-20B-NEO-CODEPlus-Uncensored-IQ4_NL.gguf"
ENV MODELS=$BAKE_MODELS

# 3. Use the existing utils.py to download the model into the image layer
# We enable hf_transfer for maximum speed during the build.
# We also clean up the Hugging Face cache folder immediately after to keep 
# the image size from doubling (only the file in /models will remain).
RUN HF_HUB_ENABLE_HF_TRANSFER=1 python3 /app/utils.py /models && \
    rm -rf /root/.cache/huggingface

ENV MODEL_DIR=/models
